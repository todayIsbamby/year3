# -*- coding: utf-8 -*-
"""4_3 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ZGDX9mdFEqTuKIyTZF2OPyurbbWxlTf
"""

!pip install scikeras

!pip install tensorflow==2.12

#Array, image processing
import cv2
import numpy as np
import matplotlib.pyplot as plt
#Model Operation
from keras import Model, Input
import keras.utils as image
from keras.wrappers.scikit_learn import KerasRegressor


from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.optimizers import Adam, SGD

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
# io
import glob
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
from google.colab import drive
drive.mount('/content/drive')

# Place tensors on the CPU
with tf.device('/GPU:0'):
    # tf.constant(image_files)
    image_files = glob.glob("/content/drive/MyDrive/datasetimgs/face_mini/**/*.jpg")  # เปลี่ยนเส้นทางไปยังโฟลเดอร์ภาพของคุณ
    imgs = []
    for i in range(1000):
        img = load_img(image_files[i], target_size=(80, 80), interpolation="nearest")

        img_array = img_to_array(img)
        imgs.append(img_array)
    print(len(imgs))
    # 2. Normalized ภาพ (เพื่อให้ค่า pixel intensity = [0, 1])
    imgs = np.array(imgs) / 255.0

    # 3. Append images to an array

    # 4. แบ่งชุดข้อมูลเป็น Training_data, Testing_data (70 : 30)
    train_x, test_x = train_test_split(imgs, random_state=42, test_size=0.3)

    # 5. แบ่งชุดข้อมูล Training_data เป็น Training_data, Validation_data (80:20)
    train_x, val_x = train_test_split(train_x, random_state=42, test_size=0.2)

    # 6. กำหนด noise parameters
    noise_mean = 0
    noise_std = 1
    noise_factor = 0.2

    # 7. สร้าง noise บวกเพิ่มเข้าในภาพ train_x, val_x, test_x
    train_x_noise = train_x + (noise_factor * np.random.normal(loc=noise_mean, scale=noise_std, size=train_x.shape))
    val_x_noise = val_x + (noise_factor * np.random.normal(loc=noise_mean, scale=noise_std, size=val_x.shape))
    test_x_noise = test_x + (noise_factor * np.random.normal(loc=noise_mean, scale=noise_std, size=test_x.shape))

print(len(image_files))

def create_autoencoder(optimizer,learning_rate):
    Input_img = Input(shape=(80,80,3))

    # Encoding architecture
    x1 = Conv2D (256, (3, 3), activation='relu', padding='same')(Input_img)
    x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)
    x3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x2)
    encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x3)

    # Decoding architecture
    x4 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)
    x5 = Conv2D(128, (3, 3), activation='relu', padding='same')(x4)
    x6 = UpSampling2D(size=(2, 2))(x5)
    x7 = Conv2D(256, (3, 3), activation='relu', padding='same')(x6)
    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x7)
    autoencoder = Model(Input_img, decoded)
    if optimizer == Adam:
        optimizer = Adam(learning_rate=learning_rate)
    elif optimizer == SGD:
        optimizer = SGD(learning_rate=learning_rate)

    autoencoder.compile(optimizer=optimizer, loss='mse')

    return autoencoder

#สร้ําง KerasRegressor Model เพื่อท ําหน้ําที่เชื่อมพํารํามิเตอร์จําก GridSearchCV ไปยัง ฟังก์ชั่นสร้ําง autoencoder
model = KerasRegressor(build_fn=create_autoencoder, epochs=2, batch_size=16, verbose=0)

#สร้ําง Dict ของชุดพํารํามิเตอร์เงื่อนไขพํารํามิเตอร์ตํามตํารํางหน้ําท้ํายเอกสําร
opts = [Adam,SGD]
lnR =[0.0001]
bs = [8,16]
eps = [150, 200]
param_grid = dict(batch_size=bs, epochs=eps, optimizer=opts,learning_rate=lnR)

#สร้ําง parameter set (ตํามเงื่อนไขใน param_grid) และจัดกําร Cross Validation ด้วย GridSearchCV

grid = GridSearchCV(estimator=model, n_jobs=1, verbose= 10, cv=2, param_grid=param_grid)

#รัน grid search เพื่อ train และ ค้นหําพํารํามิเตอร์ที่ดีที่สุด เพื่อลด noise ในภําพอินพุท เพื่อให้ได้ใกล้เคียง ภําพเป้ําหมํายที่ไม่มี noise

grid_result = grid.fit(train_x_noise, train_x)

#แสดงพํารํามิเตอร์และ score หรือ error ที่ให้ผลลัพธ์ validation ที่ดีที่สุด

grid_result.best_params_ , grid_result.best_score_

#แสดงสถิติค่ําเฉลี่ย (mean) และค่ําเบี่ยงเบนมําตรฐําน (stds) ของค่ํา score ในแต่ละค่ําพํารํามิเตอร์
print('Best params: ',grid_result.best_params_)
print('Best score: ', grid_result.best_score_)
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for i in range(3):
  print(means[i],stds[i],params[i])

#สร้ําง parameter set (ตํามเงื่อนไขใน param_grid) และจัดกําร Cross Validation ด้วย GridSearchCV

grid = GridSearchCV(estimator=model, n_jobs=1, verbose= 10, cv=4, param_grid=param_grid)

#รัน grid search เพื่อ train และ ค้นหําพํารํามิเตอร์ที่ดีที่สุด เพื่อลด noise ในภําพอินพุท เพื่อให้ได้ใกล้เคียง ภําพเป้ําหมํายที่ไม่มี noise

grid_result = grid.fit(train_x_noise, train_x)

#แสดงพํารํามิเตอร์และ score หรือ error ที่ให้ผลลัพธ์ validation ที่ดีที่สุด

#แสดงสถิติค่ําเฉลี่ย (mean) และค่ําเบี่ยงเบนมําตรฐําน (stds) ของค่ํา score ในแต่ละค่ําพํารํามิเตอร์
print('Best params: ',grid_result.best_params_)
print('Best score: ', grid_result.best_score_)
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
# Get the top 3 results

top_results_indices = np.argsort(means)[:3]
print("Top 3 results:")
for i in top_results_indices:
    print(i)
    print(f"Mean Test Score: {means[i]}, Std Test Score: {stds[i]}, Params: {params[i]}")



# top2
b= 8
e=150
l=0.0001
o = SGD
autoencoder = create_autoencoder(o,l)
callback = EarlyStopping(monitor='loss', patience=3)
history = autoencoder.fit(train_x_noise, train_x, epochs=e, batch_size=b, shuffle=True, validation_data=(val_x_noise, val_x), callbacks=[callback])

test_predictions = autoencoder.predict(test_x_noise)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss(top2)')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# top3
b= 8
e=150
l=0.0001
o = SGD
autoencoder = create_autoencoder(o,l)
callback = EarlyStopping(monitor='loss', patience=3)
history = autoencoder.fit(train_x_noise, train_x, epochs=e, batch_size=b, shuffle=True, validation_data=(val_x_noise, val_x), callbacks=[callback])

test_predictions = autoencoder.predict(test_x_noise)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss(top3)')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


best_params = grid_result.best_params_.values()
b,e,l,o = best_params
# autoencoder = Model(Input_img, decoded)
# autoencoder.compile(optimizer=o, loss='mse')
autoencoder = create_autoencoder(o,l)

callback = EarlyStopping(monitor='loss', patience=3)
history = autoencoder.fit(train_x_noise, train_x, epochs=e, batch_size=b, shuffle=True, validation_data=(val_x_noise, val_x), callbacks=[callback])

test_predictions = autoencoder.predict(test_x_noise)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss(top1)')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


# autoencoder = Model(Input_img, decoded)
# autoencoder.compile(optimizer=o, loss='mse')

#อธิบายกราฟ
#2. ผลลัพธ์ของภาพทดสอบ (Test image) มีลักษณะเป็นอย่างไร คิดว่าโครงสร้าง autoencoder เพียงพอจะได้ผลลัพธ์ที่ดีแล้วหรือไม่ เพราะเหตุใด

#  คิดว่าโครงสร้างAutoencoder สามารถทำการลดรายละเอียดในข้อมูลในขณะเดียวกันรักษาข้อมูลสำคัญไว้ได้ดีอยู่ แต่ผลลัพธ์ไม่ได้ดีมากขนาดนั้นค่ะ ซึ่งจากการทดลองเนี่ย
# การเลือกใช้ best params มาลองเทสดู 3 อันดับ ก็จะเห็นได้ว่า อันดับสองและสาม มีการใช้เวลารันนานกว่า ถึงแม้ค่า epochs จะน้อยกว่าของอันดับหนึ่ง
# แต่จะเห็นได้ว่า้เวลานานกว่า แต่จากการทดลองทั้ง cv2 และ cv8 จะเห็นว่า best params คือตัวที่ optimizer เป็น Adam
#  และเพราะว่าอาจจะมีปัญหาด้านโครงสร้างของโมเดล Autoencoder หรือตัวอัปเดต hyperparameter ที่ต้องปรับแต่งเพิ่มเติม
#  เช่น เพิ่มจำนวน epochs, เปลี่ยน learning rate, หรือเปลี่ยนโครงสร้างของ Autoencoder โดยการเพิ่มหรือลดจำนวนของชั้น (layers)
#  หรือเพิ่มขนาดของชั้น hidden layers ตามความเหมาะสม

# เลือก 3 รูปจาก test_x_noise และ test_x
num_samples = 3
selected_indices = np.random.choice(len(test_x_noise), num_samples, replace=False)

# สร้าง subplot สำหรับแสดงภาพ
plt.figure(figsize=(10, 5))

for i, index in enumerate(selected_indices):
    # กำหนด subplot ในตำแหน่งที่เหมาะสม
    plt.subplot(3, num_samples, i + 1)
    plt.imshow(test_x[index], cmap='gray')
    plt.title('Input')
    plt.axis('off')

    plt.subplot(3, num_samples, num_samples + i + 1)
    plt.imshow(test_x_noise[index], cmap='gray')
    plt.title('Noise')
    plt.axis('off')

    plt.subplot(3, num_samples, 2 * num_samples + i + 1)
    plt.imshow(test_predictions[index], cmap='gray')
    plt.title('Testing Image')
    plt.axis('off')

plt.tight_layout()
plt.show()